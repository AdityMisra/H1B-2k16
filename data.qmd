# Data

## Description

### **Data Collection and Source**

-   **Collected By**: This data is collected by the Office of Foreign Labor Certification (OFLC) under the U.S. Department of Labor.

-   **How Data is Collected**:

    -   Data is extracted from nonimmigrant and immigrant application tables in the OFLC's case management systems.

    -   Employers submit applications, and the information from these forms is entered into the system, including metadata generated during processing (e.g., submission and decision dates).

-   **Source Access**:

    -   This dataset is publicly available via the OFLC website and is distributed in Excel (.xlsx) format.

    -   Download link for historical data: https://www.dol.gov/sites/dolgov/files/ETA/oflc/pdfs/H-1B_Disclosure_Data_FY17.xlsx

    -   Dataset fields description: <https://www.dol.gov/sites/dolgov/files/ETA/oflc/pdfs/H-1B_FY17_Record_Layout.pdf>

------------------------------------------------------------------------

### **Data Format**

-   **File Type**: Microsoft Excel (.xlsx).

-   **Organization**:

    -   This dataset is cumulative for the fiscal year (October 1 – September 30).

    -   It contains records with unique identifiers (e.g., OFLC case numbers).

-   **Fields Included**:

    -   Employer-provided data (e.g., job title, worksite location, prevailing wage, and visa type).

    -   Metadata generated by the system (e.g., case status, received date, and determination date).

------------------------------------------------------------------------

### **Frequency of Updates**

-   The data is updated quarterly and annually.

-   The H1-B 2017 dataset specifically covers cases for the fiscal year 2017 (October 1, 2016 – September 30, 2017) and will not receive further updates.

------------------------------------------------------------------------

### **Data Dimensions**

-   **Number of Records**: Depends on the fiscal year's total applications, in this case its 624651 rows.

-   **Columns/Attributes**: 52 columns.

------------------------------------------------------------------------

### **Data Issues and Challenges**

-   **Known Issues**:

    -   **Data Quality**: Typographical errors and incomplete fields due to employer input errors or processing limitations.

    -   **Standardization**: Some fields might have inconsistent formats (e.g., job titles, worksite addresses or wages).

    -   **Omissions**: Records might exclude certain case details if the application is still under processing or if it includes open-ended text.

------------------------------------------------------------------------

### **Data Import Plan**

-   **Download**: The data from the OFLC website will be downloaded using a R script as an xlsx file.

------------------------------------------------------------------------

### **Documentation of Sources**

-   Primary Source: **U.S. Department of Labor - Office of Foreign Labor Certification H-1B Disclosure Data**

    -   Website: https://www.dol.gov/agencies/eta/foreign-labor/performance

    -   H-1B data file for 2017: https://www.dol.gov/sites/dolgov/files/ETA/oflc/pdfs/H-1B_Disclosure_Data_FY17.xlsx

## Missing value analysis

```{r}
# Load required libraries
#if (!require("readxl")) install.packages("readxl", dependencies = TRUE)
#if (!require("dplyr")) install.packages("dplyr", dependencies = TRUE)

library(readxl)
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
```

```{r}
# The URL for the H1-B 2017 dataset
url <- "https://www.dol.gov/sites/dolgov/files/ETA/oflc/pdfs/H-1B_Disclosure_Data_FY17.xlsx"

# The destination of  file path for the Excel file
file_path <- "H-1B_Disclosure_Data_FY17.xlsx"

# Download the file
download.file(url, destfile = file_path, mode = "wb")

h1b_data <- read_excel(file_path)

#head(h1b_data)

write.csv(h1b_data, file = "H1B_2017_cleaned.csv", row.names = FALSE)
```

```{r}
# Read the data
#h1b_data <- read_csv("H1B_2017_cleaned.csv")

# Calculate missing value counts
missing_values_summary <- colSums(is.na(h1b_data))

missing_columns <- missing_values_summary[missing_values_summary > 0]

missing_table <- data.frame(
  Column = names(missing_columns),
  MissingCount = as.numeric(missing_columns)
)

sorted_table <- missing_table %>%
  arrange(desc(MissingCount))

print(sorted_table)
```

The above table shows the missing values in each column respectively. We can see that missing values in some of the rows is huge (some of them are nearly empty). For example, Public Discolsure location is completely empty. It is possible that these values are not mandatory while filling the H-1B application, therefore leading to lot of missing information.

```{r, fig.width=8, fig.height=8}

# Plotting the percentage of missing values per column, sorted by descending missing percentage
missing_summary <- colSums(is.na(h1b_data)) / nrow(h1b_data) * 100
missing_df <- data.frame(Column = names(missing_summary), MissingPercentage = missing_summary)

ggplot(missing_df, aes(x = reorder(Column, MissingPercentage), y = MissingPercentage)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Percentage of Missing Values per Column",
    x = "Columns",
    y = "Missing Percentage (%)"
  ) +
  theme_minimal()

```

The above horizontal bar graph gives a relative view of missing values in all the columns of the dataset. Missing values can, of course, complicate the analysis; however, in our case, the columns with a high number of missing values are not critical to our primary objectives. The data in these columns—such as PUBLIC_DISCLOSURE_LOCATION, EMPLOYER_PHONE_EXT, and EMPLOYER_PROVINCE—serve as supporting information rather than core data required for analysis.

For example, columns like PUBLIC_DISCLOSURE_LOCATION only provides metadata about the file's storage location, which is not directly related to analyzing H-1B visa trends or employer statistics. Similarly, fields such as EMPLOYER_PHONE_EXT and EMPLOYER_PROVINCE primarily address contact details, which are not relevant for our analysis.

```{r, fig.width=10, fig.height=10}
set.seed(42)
sampled_data <- h1b_data[sample(1:nrow(h1b_data), 5000), ]

missing_long <- as.data.frame(is.na(sampled_data)) %>%
  pivot_longer(cols = everything(), names_to = "Column", values_to = "IsMissing")

missing_long <- cbind(RowIndex = rep(1:nrow(sampled_data), times = ncol(sampled_data)), missing_long)

# Plotting heatmap of missing data
ggplot(missing_long, aes(x = Column, y = RowIndex, fill = IsMissing)) +
  geom_tile() +
  scale_fill_manual(values = c("FALSE" = "white", "TRUE" = "red"), name = "Missing") +
  labs(title = "Heatmap of Missing Data (Sampled Rows)",
       x = "Columns",
       y = "Row Index") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The above missing value heatmap shows that missing data is concentrated only in certain columns. The rest of the columns have little to no missing data. This helps us streamline the data cleaning process.

```{r}
# Group missingness by CASE_STATUS and calculate the average missing count
grouped_missing <- h1b_data %>%
  mutate(MissingCount = rowSums(is.na(.))) %>%
  group_by(CASE_STATUS) %>%
  summarise(AverageMissing = mean(MissingCount)) %>%
  arrange(desc(AverageMissing)) # Optional: Sort for better visualization

# Plotting grouped missingness
ggplot(grouped_missing, aes(x = reorder(CASE_STATUS, -AverageMissing), y = AverageMissing)) +
  geom_bar(stat = "identity", fill = "orange") +
  labs(title = "Average Missing Values by CASE STATUS",
       x = "CASE STATUS",
       y = "Average Missing Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

This chart visualizes the average count of missing values for different categories of the column CASE_STATUS which represents the status of H-1B visa application.

The average missing values are similar across all statuses indicating no major variation in the amount of missing data among different application statuses. This implies that to handle the missing data, this graph shows that removing missing values wont introduce bias towards a specific case status.

```{r, fig.width=20, fig.height=20}
numeric_data <- h1b_data %>%
  select_if(is.numeric)

standardized_data <- as.data.frame(scale(numeric_data))

set.seed(42) 
subset_data <- standardized_data[sample(1:nrow(standardized_data), 1000), ]

subset_data <- subset_data %>%
  mutate(RowIndex = 1:nrow(.)) %>%
  pivot_longer(cols = -RowIndex, names_to = "Variable", values_to = "Value")

ggplot(subset_data, aes(x = Variable, y = RowIndex, fill = Value)) +
  geom_tile() +
  scale_fill_gradient2(low = "blue", mid = "white", high = "black", midpoint = 0) +
  labs(title = "Heatmap of Standardized Variables (Subset of Rows)",
       x = "Variable",
       y = "Row Index",
       fill = "Standardized Value") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))


```

The heatmap displayed is a visualization of standardised values in the dataset for missing values analysis. No visible pattern was found on the plotting of this graph indicating that there is no unique relationship between missing values of the columns. Additionally we also observe a fading effect when we take large number of rows. This is due to data being more homogenous, causing the the standarised values to cluster closer to 0. Some of the columns are completely white, indicating high number of missing values in those columns.

## **Data Pre-processing**

Based on our results above we have decided to remove following columns from our data-set as they are either not useful to us or have a lot of missing values.

The columns that we are dropping:

EMPLOYER_BUSINESS_DBA, EMPLOYER_ADDRESS, EMPLOYER_PROVINCE, EMPLOYER_PHONE, EMPLOYER_PHONE_EXT, ORIGINAL_CERT_DATE, AGENT_REPRESENTING_EMPLOYER, AGENT_ATTORNEY_NAME, AGENT_ATTORNEY_CITY, AGENT_ATTORNEY_STATE, PUBLIC_DISCLOSURE_LOCATION, LABOR_CON_AGREE.

```{r}
columns_to_remove <- c(
  "EMPLOYER_BUSINESS_DBA", 
  "EMPLOYER_ADDRESS", 
  "EMPLOYER_PROVINCE", 
  "EMPLOYER_PHONE", 
  "EMPLOYER_PHONE_EXT",
  "ORIGINAL_CERT_DATE",
  "AGENT_REPRESENTING_EMPLOYER",
  "AGENT_ATTORNEY_NAME", 
  "AGENT_ATTORNEY_CITY", 
  "AGENT_ATTORNEY_STATE", 
  "PUBLIC_DISCLOSURE_LOCATION", 
  "LABOR_CON_AGREE"
)

# Removing the specified columns
h1b_data_cleaned <- h1b_data |> 
  select(-all_of(columns_to_remove))

write.csv(h1b_data_cleaned, file = "H1B_2017_cleaned.csv", row.names = FALSE)

#head(h1b_data_cleaned)
```
